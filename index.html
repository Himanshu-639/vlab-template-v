<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="description" content="brief description" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Text Based Image Comparison</title>
  <link rel="icon" href="images/vlab-logo.png" type="image/x-icon" />
  <link rel="stylesheet" href="styles.css" />
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/base16/harmonic16-dark.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.8/clipboard.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>

<body>
  <header class="header">
    <div class="logo-container">
      <div class="logo">
        <a href="https://www.vlab.andcollege.du.ac.in" rel="noopener" target="_blank">
          <img class="logo-img" src="images/logo.png" alt="VLab" />
        </a>
      </div>
      <h1 class="vlab">V-Lab@ANDC</h1>
    </div>

    <div class="menu-toggle" onclick="toggleMenu()">☰</div>
    <nav class="nav-menu" id="small-screen">
      <div class="close-btn" onclick="toggleMenu()">✖</div>
      <a href="https://www.vlab.andcollege.du.ac.in">Home</a>
      <a href="https://www.vlab.andcollege.du.ac.in#labs_section">Labs</a>
      <a href="https://www.vlab.andcollege.du.ac.in#team">Team</a>
      <a href="https://www.andcollege.du.ac.in">College Website</a>
    </nav>
  </header>

  <div class="yourvlabtitle">
    <h1>Text Based Image Comparison</h1>
  </div>

  <div class="pageview">
    <nav class="navigation">
      <button class="link" onclick="switchContent('aim')">
        <img class="icon" src="images/Aim_img.png" alt="Aim" />
        <span>Aim</span>
      </button>

      <button class="link" onclick="switchContent('theory')">
        <img class="icon" src="images/theory_img.png" alt="THEORY" />
        <span>Theory & Appli.</span>
      </button>

      <button class="link" onclick="switchContent('procedure')">
        <img class="icon" src="images/procedure_img.png" alt="PROCEDURE" />
        <span>Procedure</span>
      </button>

      <button class="link" onclick="switchContent('practice')">
        <img class="icon" src="images/practice_img.png" alt="PRACTICE" />
        <span>Practice</span>
      </button>

      <button class="link" onclick="switchContent('code')">
        <img class="icon" src="images/code_img.png" alt="CODES" />
        <span>Codes</span>
      </button>

      <button class="link" onclick="switchContent('result')">
        <img class="icon" src="images/result_img.png" alt="RESULT" />
        <span>Result</span>
      </button>

      <button class="link" onclick="switchContent('quiz')">
        <img class="icon" src="images/quiz_img.png" alt="QUIZ" />
        <span>Quiz</span>
      </button>

      <button class="link" onclick="switchContent('references')">
        <img class="icon" src="images/reference_img.png" alt="REFERENCE" />
        <span>References</span>
      </button>
      <button class="link" onclick="switchContent('tnt')">
        <img class="icon" src="images/tnt_img.png" alt="TEAM & TOOLS" />
        <span>Team & Tools</span>
      </button>
    </nav>

    <section class="main_practical">
      <!-- Aim -->
      <div class="container" id="aim">
        <div class="title">Aim</div>
        <div class="content">
          <p>The aim of this Virtual Lab is to demonstrate the process of extracting text from two images using OCR and
            comparing their content using Natural Language Processing techniques to determine their textual similarity
          </p>
        </div>
      </div>

      <!-- Theory & Applications -->
      <div class="container" id="theory">
        <div class="title">Theory & Applications</div>
        <div class="content">
          <p><br>
            This experiment demonstrates the integration of <b>Optical Character Recognition (OCR)</b> and
            <b>Natural Language Processing (NLP)</b> techniques to compute the textual similarity between
            two images. The process involves extracting text from image files, preprocessing the text,
            converting it into a weighted vector representation using <b>TF-IDF</b>, and finally measuring
            their similarity using <b>Cosine Similarity</b>.
          </p>

          <span>Key Concepts</span>
          <ul>
            <li><b>Optical Character Recognition (OCR)</b>
              <ul>
                <li><b>Definition:</b> OCR is a computer vision technique used to identify and extract
                  alphanumeric text from images using pattern recognition, convolutional models, or
                  deep learning techniques.</li>
                <li><b>Functionality:</b> It converts pixel-level image data into text by recognizing
                  character shapes and structures.</li>
                <li><b>In this project:</b> OCR is performed using the <b>OCR.space API</b>, which provides
                  high-accuracy text extraction, including support for some handwritten content.
                  While open-source libraries like <b>Tesseract</b> and <b>EasyOCR</b> can also be used
                  for this task, they are primarily optimized for printed or clearly structured text
                  and often underperform on real-world handwritten images. OCR.space gives better results
                  due to its training on diverse datasets.</li>
              </ul>
            </li>

            <li><b>Natural Language Processing (NLP)</b>
              <ul>
                <li><b>Definition:</b> NLP refers to the set of algorithms and processes that allow machines
                  to understand, interpret, and manipulate human language.</li>
                <li><b>Text Preprocessing:</b> A crucial stage that prepares raw text for similarity analysis.
                  It includes:
                  <ul>
                    <li><b>Lowercasing:</b> Converts all characters to lowercase to standardize the text.</li>
                    <li><b>Handling Punctuation & Special Characters:</b> Stripped to avoid irrelevant variation
                      between texts.</li>
                    <li><b>Tokenization:</b> The process of breaking text into smaller units called tokens
                      (typically words). For example, "Text similarity project" → ["text", "similarity", "project"]</li>
                    <li><b>Stopword Removal:</b> Removes common words (like “is”, “the”, “of”) that carry little
                      semantic weight and don't contribute significantly to similarity measures.</li>
                    <li><b>Lemmatization:</b> Reduces each word to its base or dictionary form, which ensures that related terms (e.g., "jumps", "jumping", "jumped") are treated as the same core concept. This improves semantic matching</li>
                  </ul>
                </li>
              </ul>
            </li>

            <li><b>TF-IDF (Term Frequency – Inverse Document Frequency)</b>
              <ul>
                <li><b>Purpose:</b> Converts textual data into numerical feature vectors that reflect term
                  importance in a document relative to a collection.</li>
                <li><b>Term Frequency (TF):</b> Measures how frequently a term appears in a document:
                  <br><code>TF(t) = (Number of times term t appears in a document) / (Total terms in document)</code>
                </li>
                <li><b>Inverse Document Frequency (IDF):</b> Reduces the weight of common terms by:
                  <br><code>IDF(t) = log(Total number of documents / Number of documents with term t)</code>
                </li>
                <li><b>TF-IDF Score:</b>
                  <br><code>TF-IDF(t) = TF(t) × IDF(t)</code>
                </li>
                <li>This representation allows us to model documents as vectors in a high-dimensional space
                  where each dimension corresponds to a term’s weight.</li>
              </ul>
            </li>

            <li><b>Cosine Similarity</b>
              <ul>
                <li><b>Definition:</b> Cosine similarity is a metric used to measure how similar two
                  vectors are, regardless of their magnitude.</li>
                <li><b>Mathematical Formula:</b>
                  <br><code>
                  Cosine Similarity = (A · B) / (||A|| × ||B||)
                  </code><br>
                  where A and B are the TF-IDF vectors of two documents.
                </li>
                <li><b>Interpretation:</b> The result ranges from 0% to 100%:
                  <ul>
                    <li>1 → identical direction → highly similar</li>
                    <li>0 → orthogonal → completely dissimilar</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>

          <span>Applications in Real World</span>
          <ol>
            <li>
              <b>Text-Based Image Comparison for Academic Integrity:</b>
              Educational platforms can use this system to compare scanned handwritten or printed assignments
              to detect content overlap using cosine similarity on preprocessed TF-IDF vectors — useful for
              plagiarism detection where traditional text matching tools fail.
            </li>
            <li>
              <b>Duplicate Document Detection in Digital Archives:</b>
              In large archives of scanned documents (legal files, forms, contracts), this technique can identify
              duplicate or near-duplicate text content across differently formatted or handwritten copies by
              comparing their TF-IDF representations.
            </li>
            <li>
              <b>Automated Metadata Tagging for Scanned Content:</b>
              When indexing a large number of scanned pages, NLP can be used to extract and summarize dominant
              terms using TF-IDF weights. These terms can be used to automatically generate tags or categories.
            </li>
            <li>
              <b>Evidence Matching in Digital Forensics:</b>
              Investigators can analyze and compare handwritten notes or printed letters using OCR and cosine similarity
              to find textual overlaps, especially when content is disguised or reformatted.
            </li>
            <li>
              <b>Document Clustering and Organization:</b>
              By generating TF-IDF vectors from scanned images, documents can be grouped into clusters
              (e.g., job applications, receipts, letters) based on textual similarity without manual sorting.
            </li>
          </ol>
        </div>
      </div>

      <!-- Procedure -->
      <div class="container" id="procedure">
        <div class="title">Procedure</div>
        <div class="content">
          <ol>
            <li><b>Upload Input Images</b>
              <ul>
                <li>Select two image files that contain textual content (e.g., typed documents, printed forms, or
                  handwritten notes).</li>
                <li>The interface provides two separate upload fields for Image 1 and Image 2.</li>
              </ul>
            </li>

            <li><b>Perform OCR on Uploaded Images</b>
              <ul>
                <li>Click the <i>"Extract Text"</i> button.</li>
                <li>Each image is sent to the OCR.space API.</li>
                <li>The API returns the extracted text for both images separately.</li>
                <li>The extracted text is displayed on the screen for verification.</li>
              </ul>
            </li>

            <li><b>Preprocess the Extracted Text</b>
              <ul>
                <li>Click the <i>"Preprocess Text"</i> button.</li>
                <li>The following preprocessing steps are applied to each extracted text:
                  <ul>
                    <li>Lowercasing</li>
                    <li>Removing punctuation and special characters</li>
                    <li>Tokenization (splitting text into words)</li>
                    <li>Stopword removal (removing common words like "the", "and", "is")</li>
                    <li>Lemmatization (reducing each word to its base or dictionary form, e.g., "running" → "run")</li>
                  </ul>
                </li>
                <li>The cleaned and tokenized text is then shown on the interface.</li>
              </ul>
            </li>

            <li><b>Vectorize the Text using TF-IDF</b>
              <ul>
                <li>The preprocessed texts from both images are converted into TF-IDF vectors.</li>
                <li>Each term is assigned a numerical weight based on its importance in the document and rarity across
                  documents.</li>
              </ul>
            </li>

            <li><b>Calculate Cosine Similarity</b>
              <ul>
                <li>Click the <i>"Calculate Similarity"</i> button.</li>
                <li>Cosine Similarity is calculated using the TF-IDF vectors:
                  <br>&nbsp;&nbsp;&nbsp; <code>Cosine Similarity = (A · B) / (||A|| × ||B||)</code>
                </li>
                <li>A similarity score between 0 and 1 is computed and displayed.</li>
                <li>The score indicates how similar the two images are based on their text content.</li>
              </ul>
            </li>

            <li><b>Interpret the Result</b>
              <ul>
                <li>A higher similarity score (close to 100%) means the two texts are very similar or identical.</li>
                <li>A lower score (close to 0%) means the texts are largely different.</li>
                <li>Students can upload different images to explore and analyze how text content affects similarity.
                </li>
              </ul>
            </li>
          </ol>

        </div>
      </div>

      <!-- Code -->
      <div class="container" id="code">
        <div class="title">Code</div>
        <p>Example programs</p>
        <div class="switch-container">
          <input class="togswt" type="radio" id="cppRadio" name="codeSwitch" checked />
          <label for="cppRadio">C++</label>

          <input class="togswt" type="radio" id="pythonRadio" name="codeSwitch" />
          <label for="pythonRadio">Python</label>
        </div>

        <div class="code-blocks">
          <div id="cppCode" class="code-block active">
            <div class="code-content">
              <pre><code class="language-cpp">// C++ Code Example</code></pre>
              <button class="copy-button">Copy</button>
            </div>
          </div>
          <div id="pythonCode" class="code-block">
            <div class="code-content">
              <pre><code class="language-python">
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import requests
import os
import re
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from dotenv import load_dotenv
load_dotenv()

OCR_API_KEY = os.getenv("OCR_API_KEY")

app = Flask(__name__)
CORS(app)

UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# Function to call the OCR.space API
def ocr_space_file(filename):
    payload = {
        'isOverlayRequired': False,
        'apikey': OCR_API_KEY,
        'language': 'eng',
        'OCREngine': 2
    }
    with open(filename, 'rb') as f:
        response = requests.post('https://api.ocr.space/parse/image', 
                                 files={filename: f}, 
                                 data=payload)
    return response.json()

@app.route('/')
def index():
    return render_template('index.html')

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
stopwords_english = set(stopwords.words('english'))
additional_punctuation = set(string.punctuation)

# Clean unwanted characters from the OCR extracted text
def clean_extracted_text(text):
    cleaned_text = re.sub(r'[→V]', '', text)
    return cleaned_text.strip()

# Preprocess text: lowercase, remove stopwords & punctuation
def preprocess_text(text):
    # Lowercase and clean text
    text = text.lower()
    text = clean_extracted_text(text)
    tokens = word_tokenize(text)
    
    # Remove stopwords and punctuation
    processed_tokens = [token for token in tokens if token not in stopwords_english and token not in additional_punctuation]
    
    # Join tokens back into a string
    processed_text = " ".join(processed_tokens)
    return processed_text

# Compare two texts using TF-IDF and cosine similarity
def compare_tfidf_cosine(text1, text2):
    # Preprocess the texts
    processed_text1 = preprocess_text(text1)
    processed_text2 = preprocess_text(text2)
    print(processed_text1, "\n")
    print(processed_text2, "\n")
    # Vectorize texts with TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])
    
    # Compute cosine similarity (returns a 2x2 matrix)
    cosine_sim_matrix = cosine_similarity(tfidf_matrix)
    print(cosine_sim_matrix, "\n")
    similarity = cosine_sim_matrix[0][1] * 100  # convert to percentage
    print("Similarity : ", similarity)
    return round(similarity, 2)

@app.route("/api/upload/", methods=["POST"])
def upload_files():
    if "image1" not in request.files or "image2" not in request.files:
        return jsonify({"error": "Both images are required"}), 400
    
    image1 = request.files["image1"]
    image2 = request.files["image2"]

    image1_path = os.path.join(app.config['UPLOAD_FOLDER'], "image1.png")
    image2_path = os.path.join(app.config['UPLOAD_FOLDER'], "image2.png")

    image1.save(image1_path)
    image2.save(image2_path)

    result1 = ocr_space_file(image1_path)
    result2 = ocr_space_file(image2_path)

    print("OCR Result 1:", result1)
    print("OCR Result 2:", result2)

    text1 = result1.get("ParsedResults", [{}])[0].get("ParsedText", "")
    text2 = result2.get("ParsedResults", [{}])[0].get("ParsedText", "")

    if not text1 or not text2:
        return jsonify({"error": "Failed to extract text from images"}), 500

    similarity_score = compare_tfidf_cosine(text1, text2)
    print("Similarity Score:", similarity_score)

    return jsonify({"similarity_score": similarity_score})

if __name__ == "__main__":
    app.run(debug=True)

              </code></pre>
              <button class="copy-button">Copy</button>
            </div>
          </div>
        </div>
      </div>

      <!-- Practice -->
      <div class="container" id="practice">
        <div class="content">

          <!-- Tab Buttons -->
          <div class="tab-bar">
            <button data-target="example1Wrapper">▶ Example 1</button>
            <button data-target="example2Wrapper">▶ Example 2</button>
            <button data-target="practiceWrapper">▶ Try Yourself</button>
          </div>

          <!-- Example 1 Tab Content -->
          <div class="transition-wrapper" id="example1Wrapper">
            <div class="contain">
              <div class="step">
                <p>
                  This example demonstrates the full pipeline of the experiment using two sample images.
                  Each stage illustrates how the content is transformed from raw image data to a final similarity score.
                </p><br>
                <p>
                  The user is required to upload two images which are to be compared on the basis of the text contained in them.<br>
                  Now consider the following two images are uploaded by the user:
                </p>
                <div class="image-row">
                  <img src="images/CPU.png" alt="Image 1">
                  <img src="images/computer_architecture.jpg" alt="Image 2">
                </div>              
                <p><br>
                  After clicking the <b>“Extract Text”</b> button, each image is sent to the <b>OCR.space API</b>. 
                  The API processes the image and returns the detected text.
                </p>
                <div class="processing-step">Text Extracted</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>Input from\nDevice\nCentral Processing Unit\nControl Unit\nArithmetic/Logical Unit\nRegisters\nMemory Unit\nOutput from\nDevice</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b><code>Input Unit\nCPU\nControl Unit\nArithmetic\n& Logic Unit\nMemory Unit\nOutput Unit</code></p>
                  </div>
                </div>
              </div>
              

              <div class="step">
                <p>
                  Before comparing the extracted texts, preprocessing is essential to clean and standardize the content. 
                  This step ensures that insignificant variations (like punctuation or casing) do not affect the similarity result.
                </p>
              
                <div class="processing-step">After Lowercasing</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>input from\ndevice\ncentral processing unit\ncontrol unit\narithmetic/logical unit\nregisters\nmemory unit\noutput from\ndevice</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b> <code>input unit\ncpu\ncontrol unit\narithmetic\n& logic unit\nmemory unit\noutput unit</code></p>
                  </div>
                </div>              

                <div class="processing-step">After Punctuation Removal</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>input from\ndevice\ncentral processing unit\ncontrol unit\narithmetic logical unit\nregisters\nmemory unit\noutput from\ndevice</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b> <code>input unit\ncpu\ncontrol unit\narithmetic\nlogic unit\nmemory unit\noutput unit</code></p>
                  </div>
                </div>

                <div class="processing-step">After Tokenization</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>['input', 'from', 'device', 'central', 'processing', 'unit', 'control', 'unit', 'arithmetic', 'logical', 'unit', 'registers', 'memory', 'unit', 'output', 'from', 'device']</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b> <code>['input', 'unit', 'cpu', 'control', 'unit', 'arithmetic', 'logic', 'unit', 'memory', 'unit', 'output', 'unit']</code></p>
                  </div>
                </div>

                <div class="processing-step">After Stopword Removal</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>input device central processing unit control unit arithmetic logical unit registers memory unit output device</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b> <code>input unit cpu control unit arithmetic logic unit memory unit output unit</code></p>
                  </div>
                </div>

                <div class="processing-step">After Lemmatization</div>
                <div class="step-comparison">
                  <div class="step-box">
                    <p><b>Image 1:</b> <code>input device central process unit control unit arithmetic logical unit register memory unit output device</code></p>
                  </div>
                  <div class="step-box">
                    <p><b>Image 2:</b> <code>input unit cpu control unit arithmetic logic unit memory unit output unit</code></p>
                  </div>
                </div>
              </div>

              <div class="step">
                <p>
                  The cleaned tokens obtained after all preprocessing steps are now transformed into numerical vectors using <b>TF-IDF (Term Frequency – Inverse Document Frequency)</b>. 
                  TF-IDF helps in assigning weights to words based on their importance — terms that are frequent in a document but rare across other documents are given more significance.
                </p>

                <p><br><b>TF-IDF Score Formula (simplified):</b></p>
                <p>
                  <code>TF-IDF(t, d) = TF(t, d) × IDF(t)</code><br>
                  Where:<br>
                  – <b>TF(t, d)</b>: Term Frequency – how many times term <code>t</code> appears in document <code>d</code><br>
                  – <b>IDF(t)</b>: Inverse Document Frequency = <code>log[(N + 1) / (DF + 1)] + 1</code>
                </p><br>
                
                <p>
                  <i>Note:</i> You may notice a slight change in the formula of IDF compared to the traditional form.
                  This version — <code>log((N + 1) / (DF + 1)) + 1</code> — is commonly used in implementations like <code>sklearn</code>'s <code>TfidfVectorizer</code> to prevent division-by-zero and stabilize the values when working with small datasets. Adding 1 to both numerator and denominator is known as <b>smoothing</b>, and it helps ensure more balanced weight distribution across rare and common terms.
                </p><br>

                <p>The following matrix shows the TF-IDF weights assigned to each term in Image 1 and Image 2 after vectorization:</p>
                <div style="text-align: center;">
                  <img src="images/TFIDF.png" alt="TF-IDF Matrix" style="max-width: 100%; height: auto; border: 1px solid #ccc; padding: 6px;">
                </div>
              </div>

              <div class="step">
                <p>
                  After converting the preprocessed text from both images into TF-IDF vectors, we compute the cosine similarity between them.
                  Using Cosine Similarity Formula:
                </p>
                <pre><code>cos(θ) = (A · B) / (||A|| × ||B||)</code></pre><br>
                <p>
                  The result is presented in the form of a similarity matrix, where each element <code>(i, j)</code> represents the similarity between Document <code>i</code> and Document <code>j</code>. The matrix shown below is the actual cosine similarity matrix obtained in this case.
                </p>
                <div style="text-align: left; margin: 10px 0;">
                <p><b>Cosine Similarity Matrix:</b></p>
  <pre style="display: inline-block; background: #f8f8f8; padding: 12px; border-radius: 6px; border: 1px solid #ccc;">[[1.0       0.7073]
 [0.7073    1.0]]</pre>
                </div><br>

                <p>
                  The value <code>0.7073</code> indicates the similarity score between the two uploaded images.
                </p><br>

                <div class="result-box">
                  <h2>Similarity Score: <span style="color: green;">70.73%</span></h2>
                </div>
              </div>
            </div>
          </div>

          <!-- Example 2 Tab Content -->
          <div class="transition-wrapper" id="example2Wrapper">
            <div class="contain">
              <div class="title">Example 2</div>
              <p>This is some dummy content for Example 2. Replace with actual sample data or illustrations.</p>
            </div>
          </div>

          <!-- Practice Content (Initially Hidden) -->
          <div class="transition-wrapper" id="practiceWrapper">
          <div class="contain">
            <div class="box" id="box1">
              <div class="upload-content" id="upload-content1">
                <div class="upload-icon">📤</div>
                <h3 id="label1">Upload Image 1</h3>
                <input type="file" id="image1" accept="image/*" style="display: none;">
              </div>
              <img id="preview1" style="display:none;">
            </div>
            <div class="box" id="box2">
              <div class="upload-content" id="upload-content2">
                <div class="upload-icon">📤</div>
                <h3 id="label2">Upload Image 2</h3>
                <input type="file" id="image2" accept="image/*" style="display: none;">
              </div>
              <img id="preview2" style="display:none;">
            </div>
          </div>

          <button onclick="extractText()">Extract Text</button>

          <div class="contain">
            <div class="output-box" id="text1">Extracted Text 1</div>
            <div class="output-box" id="text2">Extracted Text 2</div>
          </div>

          <button onclick="preprocessText()">Preprocess Text</button>

          <div class="contain">
            <div class="output-box" id="pre1">Preprocessed Text 1</div>
            <div class="output-box" id="pre2">Preprocessed Text 2</div>
          </div>

          <button onclick="calculateSimilarity()">Calculate Similarity</button>

          <div class="scorecard" id="similarityResult">Similarity: Not Calculated</div>
          </div>
        </div>
      </div>

      <script>
        const imageInputs = [document.getElementById('image1'), document.getElementById('image2')];
        const previews = [document.getElementById('preview1'), document.getElementById('preview2')];
        const uploadContents = [document.getElementById('upload-content1'), document.getElementById('upload-content2')];
        const texts = [document.getElementById('text1'), document.getElementById('text2')];
        const preprocessed = [document.getElementById('pre1'), document.getElementById('pre2')];
        const base64Images = [null, null];
        const extractedTexts = ["", ""];

        imageInputs.forEach((input, index) => {
          const box = document.getElementById('box' + (index + 1));

          const handleImage = file => {
            const reader = new FileReader();
            reader.onload = function (e) {
              previews[index].src = e.target.result;
              previews[index].style.display = 'block';
              uploadContents[index].style.display = 'none';
              base64Images[index] = e.target.result.replace(/^data:image\/(png|jpg|jpeg);base64,/, '');
            };
            reader.readAsDataURL(file);
          };

          input.addEventListener('change', function () {
            if (this.files[0]) {
              handleImage(this.files[0]);
            }
          });

          ['dragenter', 'dragover'].forEach(eventName => {
            box.addEventListener(eventName, e => {
              e.preventDefault();
              box.classList.add('dragover');
            });
          });

          ['dragleave', 'drop'].forEach(eventName => {
            box.addEventListener(eventName, e => {
              e.preventDefault();
              box.classList.remove('dragover');
            });
          });

          box.addEventListener('drop', e => {
            const file = e.dataTransfer.files[0];
            if (file && file.type.startsWith('image/')) {
              handleImage(file);
            }
          });

          box.addEventListener('click', () => {
            input.value = ''; // clear previous file to avoid double trigger
            input.click();
          });
        });

        async function extractText() {
          const apiKey = 'K88181478588957';
          for (let i = 0; i < 2; i++) {
            if (!base64Images[i]) {
              texts[i].innerText = 'No image selected';
              continue;
            }
            texts[i].innerText = 'Extracting...';
            const formData = new FormData();
            formData.append('base64Image', 'data:image/jpeg;base64,' + base64Images[i]);
            formData.append('language', 'eng');
            formData.append('apikey', apiKey);
            formData.append('OCREngine', 2);
            try {
              const response = await fetch('https://api.ocr.space/parse/image', {
                method: 'POST',
                body: formData
              });
              const result = await response.json();
              const parsedText = result.ParsedResults?.[0]?.ParsedText || "No text found.";
              extractedTexts[i] = parsedText;
              texts[i].innerText = parsedText;
            } catch (error) {
              texts[i].innerText = 'Error during OCR';
            }
          }
        }

        async function preprocessText() {
  const text1 = extractedTexts[0];
  const text2 = extractedTexts[1];

  if (!text1 || !text2) {
    alert("Please extract text from both images first.");
    return;
  }

  try {
    const response = await fetch('http://127.0.0.1:5000/preprocess', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text1, text2 })
    });

    const result = await response.json();

    // Only show preprocessed (lemmatized) text
    preprocessed[0].innerText = result.step1.lemma;
    preprocessed[1].innerText = result.step2.lemma;

    // Clear similarity output for now
    document.getElementById('similarityResult').innerText = "Similarity: Not Calculated";

  } catch (error) {
    console.error("Preprocessing failed:", error);
    alert("Backend preprocessing error.");
  }
}


async function calculateSimilarity() {
  const pre1 = preprocessed[0].innerText;
  const pre2 = preprocessed[1].innerText;

  if (!pre1 || !pre2) {
    alert("Please preprocess both texts first.");
    return;
  }

  try {
    const response = await fetch('http://127.0.0.1:5000/similarity', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text1: pre1, text2: pre2 })
    });

    const result = await response.json();
    document.getElementById('similarityResult').innerText = `Similarity: ${Math.round(result.similarity * 100)}%`;

  } catch (error) {
    console.error("Similarity calculation failed:", error);
    alert("Error while calculating similarity.");
  }
}


      </script>


      <!-- Results -->
      <div class="container" id="result">
        <div class="title">Result</div>
        <div class="content">
          <p><br>
            Upon completing the steps of the experiment, a <b>similarity score</b> is generated based on the textual content extracted from the two uploaded images. This result offers a quantitative measure of how closely the images match in terms of written information.
          </p>
        
          <p>
            The similarity score is calculated using <b>TF-IDF vectorization</b> and <b>cosine similarity</b>. Each image is represented as a vector of weighted words, and the angle between these vectors is measured. The smaller the angle (i.e., the closer the vectors), the higher the similarity score.
          </p>
        
          <p><br><b>Interpretation of the result:</b></p>
          <ul>
            <li><b>Score close to 100%:</b> The texts in both images are nearly or completely identical.</li>
            <li><b>Score close to 0%:</b> The texts are entirely different with no significant overlap.</li>
            <li><b>Intermediate score:</b> The texts have partial similarity — they may share some common phrases or keywords but differ in structure or content.</li>
          </ul>
        
          <p>
            This result helps users understand the degree of similarity between scanned documents, handwritten notes, or printed forms. It highlights the impact of preprocessing (like stopword removal and tokenization) and shows how semantic closeness can be identified even when wording slightly varies.
          </p>
        
          <p><br><b>Example:</b></p>
          <p>
            - Image 1 text: <i>"Machine learning is transforming modern industries."</i><br>
            - Image 2 text: <i>"Modern industries are being changed by machine learning."</i><br>
            - Similarity Score: <code>0.82</code>
          </p>
        
          <p>
            Although the wording differs, the core idea is preserved in both sentences. The system captures this using NLP and vector-based similarity computation, demonstrating how machines can approximate human-like understanding of textual similarity.
          </p>
        </div>
      </div>

      <!-- Quiz -->
      <div class="container" id="quiz">
        <div class="title">Quiz</div>
        <p id="question"></p>
        <div id="choices" class="choices"></div>
        <button id="save-btn" class="save-button">Save</button>
        <button id="next-btn" class="next-button" style="display: none">
          Next
        </button>
        <button id="retake-btn" style="display: none">Retake Quiz</button>
        <div id="quiz-report" style="display: none"></div>
        <!-- Section to display quiz report -->
      </div>

      <!-- References -->
      <div class="container" id="references">
        <div class="title">References</div>
        <div class="content">
          <ul class="ref-list">
            <li>
              <span> refernce 1 </span>
            </li>
            <li>
              <span> refernce 2 </span>
            </li>
          </ul>
        </div>
      </div>

      <!-- Team & Tools -->
      <div class="container" id="tnt">
        <div class="title">Team & Tools</div>
        <div class="content">
          <h3>Students</h3>
          <ul class="ref-list">
            <li>
              <a href="https://www.linkedin.com/in/kriti-misra-b6014a29b" rel="noopener" target="_blank"><span>Kriti
                  Misra, BSc (Hons) Computer Science (2024-25)</span></a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/639himanshuyadav/" rel="noopener" target="_blank"><span>Himanshu
                  Yadav, BSc (Hons) Computer Science (2024-25)</span></a>
            </li>
          </ul>
          <h3>Mentors</h3>
          <ul class="ref-list">
            <li><span>Prof. Sharanjit Kaur, Department of Computer Science</span></li>
          </ul>
          <h3>Tools Used</h3>
          <ul class="tools-list">
            <li>
              <span>OCR.space API - for text extraction </span>
            </li>
            <li>
              <span>NLTK (Natural Language Toolkit) library - for text preprocessing </span>
            </li>
            <li>
              <span>TfidfVectorizer, cosine_similarity - for calculating similarity </span>
            </li>
            <li>
              <span>Vanilla HTML, CSS, JS - for creating the web page</span>
            </li>
            <li>
              <span>Flask - for backend </span>
            </li>
          </ul>
        </div>
      </div>
    </section>
  </div>
  <script>
    hljs.highlightAll();
    function toggleMenu() {
      const menu = document.querySelector(".nav-menu");
      menu.classList.toggle("show");
    }
  </script>
  <script src="script.js"></script>
</body>

</html>